{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Modeling"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that I have the data have been prepared with a minimum number of predictor variables that can represent the data very well, I can Train/Test different classification algorithms and measure the performance for each model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import libraries necessary for this part of the project\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from time import time\n",
    "\n",
    "# Allows the use of display() for DataFrames\n",
    "from IPython.display import display \n",
    "\n",
    "# Import supplementary visualizations code visuals_2.py\n",
    "import visuals_2 as vs2\n",
    "\n",
    "# Pretty display for notebooks\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the Santander Customer Satisfacations data from a CSV file\n",
    "\n",
    "data = pd.read_csv('Data/processed_dataset.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Dimension 1</th>\n",
       "      <th>Dimension 2</th>\n",
       "      <th>Dimension 3</th>\n",
       "      <th>Dimension 4</th>\n",
       "      <th>Dimension 5</th>\n",
       "      <th>Dimension 6</th>\n",
       "      <th>Dimension 7</th>\n",
       "      <th>Dimension 8</th>\n",
       "      <th>Dimension 9</th>\n",
       "      <th>Dimension 10</th>\n",
       "      <th>Dimension 11</th>\n",
       "      <th>Dimension 12</th>\n",
       "      <th>TARGET</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>-4.880790e+07</td>\n",
       "      <td>-1.090507e+07</td>\n",
       "      <td>-5.486603e+06</td>\n",
       "      <td>-3.378133e+06</td>\n",
       "      <td>305090.5665</td>\n",
       "      <td>-641634.4108</td>\n",
       "      <td>-535777.4725</td>\n",
       "      <td>-22051.9136</td>\n",
       "      <td>-23453.0686</td>\n",
       "      <td>-1.8078</td>\n",
       "      <td>-187289.5343</td>\n",
       "      <td>100374.4060</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>-4.880790e+07</td>\n",
       "      <td>-1.090507e+07</td>\n",
       "      <td>-5.486602e+06</td>\n",
       "      <td>-3.378133e+06</td>\n",
       "      <td>305090.6422</td>\n",
       "      <td>-641634.3768</td>\n",
       "      <td>-535777.5061</td>\n",
       "      <td>-22051.9036</td>\n",
       "      <td>-23453.1126</td>\n",
       "      <td>-1.7300</td>\n",
       "      <td>-187289.4573</td>\n",
       "      <td>100374.4646</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>-4.880790e+07</td>\n",
       "      <td>-1.090507e+07</td>\n",
       "      <td>-5.486602e+06</td>\n",
       "      <td>-3.378133e+06</td>\n",
       "      <td>305090.7846</td>\n",
       "      <td>-641634.3038</td>\n",
       "      <td>-535777.5623</td>\n",
       "      <td>-22051.8649</td>\n",
       "      <td>-23453.1858</td>\n",
       "      <td>-1.5902</td>\n",
       "      <td>-187289.3159</td>\n",
       "      <td>100374.5903</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>-4.880790e+07</td>\n",
       "      <td>-1.090507e+07</td>\n",
       "      <td>-5.486602e+06</td>\n",
       "      <td>-3.378133e+06</td>\n",
       "      <td>305090.7590</td>\n",
       "      <td>-641634.3164</td>\n",
       "      <td>-535777.5521</td>\n",
       "      <td>-22051.8730</td>\n",
       "      <td>-23453.1747</td>\n",
       "      <td>-1.6161</td>\n",
       "      <td>-187289.3418</td>\n",
       "      <td>100374.5661</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>-4.880790e+07</td>\n",
       "      <td>-1.090507e+07</td>\n",
       "      <td>-5.486598e+06</td>\n",
       "      <td>-3.378131e+06</td>\n",
       "      <td>305092.4727</td>\n",
       "      <td>-641635.2144</td>\n",
       "      <td>-535778.6937</td>\n",
       "      <td>-22057.0651</td>\n",
       "      <td>-23457.1630</td>\n",
       "      <td>-1.4047</td>\n",
       "      <td>-187289.1709</td>\n",
       "      <td>100369.7865</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "    Dimension 1   Dimension 2   Dimension 3   Dimension 4  Dimension 5  \\\n",
       "0 -4.880790e+07 -1.090507e+07 -5.486603e+06 -3.378133e+06  305090.5665   \n",
       "1 -4.880790e+07 -1.090507e+07 -5.486602e+06 -3.378133e+06  305090.6422   \n",
       "2 -4.880790e+07 -1.090507e+07 -5.486602e+06 -3.378133e+06  305090.7846   \n",
       "3 -4.880790e+07 -1.090507e+07 -5.486602e+06 -3.378133e+06  305090.7590   \n",
       "4 -4.880790e+07 -1.090507e+07 -5.486598e+06 -3.378131e+06  305092.4727   \n",
       "\n",
       "   Dimension 6  Dimension 7  Dimension 8  Dimension 9  Dimension 10  \\\n",
       "0 -641634.4108 -535777.4725  -22051.9136  -23453.0686       -1.8078   \n",
       "1 -641634.3768 -535777.5061  -22051.9036  -23453.1126       -1.7300   \n",
       "2 -641634.3038 -535777.5623  -22051.8649  -23453.1858       -1.5902   \n",
       "3 -641634.3164 -535777.5521  -22051.8730  -23453.1747       -1.6161   \n",
       "4 -641635.2144 -535778.6937  -22057.0651  -23457.1630       -1.4047   \n",
       "\n",
       "   Dimension 11  Dimension 12  TARGET  \n",
       "0  -187289.5343   100374.4060       0  \n",
       "1  -187289.4573   100374.4646       0  \n",
       "2  -187289.3159   100374.5903       0  \n",
       "3  -187289.3418   100374.5661       0  \n",
       "4  -187289.1709   100369.7865       0  "
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data.head(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(76020, 13)"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1- Split Data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### First : \n",
    "categoraing data to features and Target "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import train_test_split\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "features = data[['Dimension 1','Dimension 2','Dimension 3', 'Dimension 4','Dimension 5',\\\n",
    "            'Dimension 6','Dimension 7', 'Dimension 8','Dimension 9', 'Dimension 10',\\\n",
    "            'Dimension 11','Dimension 12']]\n",
    "\n",
    "satisfaction_status = data['TARGET']\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Second : \n",
    "\n",
    "Splitting the data into train and test sets, where 80% of the data will be used for training and 20% data for testing."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training set has 53214 samples.\n",
      "Testing set has 22806 samples.\n"
     ]
    }
   ],
   "source": [
    "# Split the 'calculated features' and 'TARGET' data into training and testing sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(features, \n",
    "                                                    satisfaction_status, \n",
    "                                                    test_size = 0.3, \n",
    "                                                    random_state = 42)\n",
    "\n",
    "# Show the results of the split\n",
    "print(\"Training set has {} samples.\".format(X_train.shape[0]))\n",
    "print(\"Testing set has {} samples.\".format(X_test.shape[0]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2- Modeling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Method to plot the ROC curve\n",
    "def ROC_Score (y_test , predictions_test):\n",
    "    from sklearn.metrics import roc_curve\n",
    "\n",
    "    # calculate roc curve\n",
    "    fpr, tpr, thresholds = roc_curve(y_test, predictions_test)\n",
    "\n",
    "    # plot no skill\n",
    "    plt.plot([0, 1], [0, 1], linestyle='--')\n",
    "\n",
    "    # plot the roc curve for the model\n",
    "    plt.plot(fpr, tpr, marker='.')\n",
    "    \n",
    "    # show the plot\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import two metrics from sklearn - fbeta_score and accuracy_score\n",
    "from sklearn.metrics import fbeta_score\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.metrics import roc_auc_score\n",
    "from sklearn.metrics import classification_report\n",
    "from sklearn.metrics import confusion_matrix\n",
    "\n",
    "\n",
    "# Method to train & predict & evaluate the performance\n",
    "def train_predict(learner, X_train, y_train, X_test, y_test): \n",
    "    '''\n",
    "    inputs:\n",
    "       - learner: the learning algorithm to be trained and predicted on\n",
    "       - X_train: features training set\n",
    "       - y_train: income training set\n",
    "       - X_test: features testing set\n",
    "       - y_test: income testing set\n",
    "    '''\n",
    "\n",
    "    \n",
    "    results = {}\n",
    "    \n",
    "    # Fit the learner to the training data (X_train)\n",
    "    start = time() # Get start time\n",
    "    learner = learner.fit(X_train,y_train)\n",
    "    end = time() # Get end time\n",
    "    \n",
    "    # Calculate the training time\n",
    "    results['train_time'] = end - start\n",
    "        \n",
    "    # Get the predictions on the test set(X_test)\n",
    "    # then get predictions on the train set(X_train)\n",
    "    start = time() # Get start time\n",
    "    predictions_test = learner.predict(X_test)\n",
    "    predictions_train = learner.predict(X_train)\n",
    "    end = time() # Get end time\n",
    "    \n",
    "    # Calculate the total prediction time\n",
    "    results['pred_time'] = end - start\n",
    "    # Compute accuracy on the training set\n",
    "    results['acc_train'] = accuracy_score(y_train, predictions_train)\n",
    "        \n",
    "    # Compute accuracy on test set\n",
    "    results['acc_test'] = accuracy_score(y_test, predictions_test)\n",
    "    \n",
    "    # Compute F-score on the training set\n",
    "    results['f_train'] = fbeta_score(y_train, predictions_train, beta=0.5)\n",
    "        \n",
    "    # Compute F-score on the test set \n",
    "    results['f_test'] = fbeta_score(y_test, predictions_test, beta=0.5)\n",
    "    \n",
    "    # calculate AUC\n",
    "    results['AUC']=  roc_auc_score(y_test, predictions_test)\n",
    "    \n",
    "    #confusion matrix    \n",
    "    confusion_df = pd.DataFrame(confusion_matrix(y_test, predictions_test),\n",
    "             columns=[\"Predicted Class \" + str(class_name) for class_name in [0,1]],\n",
    "             index = [\"Class \" + str(class_name) for class_name in [0,1]])\n",
    "    \n",
    "    #classification report\n",
    "    classifying_report = classification_report(y_test, predictions_test)\n",
    "\n",
    "    \n",
    "    # Success\n",
    "    print(\"{} has trained.\".format(learner.__class__.__name__))\n",
    "    print('confusion_matrix')\n",
    "    print(confusion_df)\n",
    "    print('-------------------/n')\n",
    "    print('classification_report')\n",
    "    print(classifying_report)\n",
    "    print('-------------------/n')\n",
    "    print(results)\n",
    "    print('-------------------/n')\n",
    "    ROC_Score (y_test , predictions_test)\n",
    "    print('----------------------------------------------------------------------------------/n')\n",
    "    # Return the results\n",
    "    return results\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Train and Test the following supervised learning models:\n",
    "\n",
    "1. Logistic Regression\n",
    "\n",
    "2. Decision Tree\n",
    "\n",
    "3. Random Forest\n",
    "\n",
    "4. Naive Bayes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import the four supervised learning models from sklearn\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.naive_bayes import GaussianNB\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "\n",
    "# Initialize the four models\n",
    "clf_A = RandomForestClassifier(random_state=42)\n",
    "clf_B = GaussianNB()\n",
    "clf_C = LogisticRegression(random_state=42)\n",
    "clf_D = DecisionTreeClassifier()\n",
    "\n",
    "# Collect results on the learners\n",
    "results = {}\n",
    "for clf in [clf_A, clf_B, clf_C, clf_D]:\n",
    "    clf_name = clf.__class__.__name__\n",
    "    results[clf_name] = {}\n",
    "    results[clf_name] = train_predict(clf, X_train, y_train, X_test, y_test)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run metrics visualization for the three supervised learning models chosen\n",
    "vs2.evaluate(results)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "##############################"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# LogisticRegression\n",
    "\n",
    "from sklearn.metrics import roc_curve\n",
    "from sklearn.metrics import roc_auc_score\n",
    "\n",
    "learner = clf_C.fit(X_train,y_train)\n",
    "\n",
    "predictions_test = learner.predict(X_test)  #probs\n",
    "\n",
    "# keep probabilities for the positive outcome only\n",
    "#predictions_test = predictions_test[:, 1]\n",
    "\n",
    "# calculate AUC\n",
    "auc = roc_auc_score(y_test, predictions_test)\n",
    "print('AUC: %.3f' % auc)\n",
    "\n",
    "# calculate roc curve\n",
    "fpr, tpr, thresholds = roc_curve(y_test, predictions_test)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# plot no skill\n",
    "plt.plot([0, 1], [0, 1], linestyle='--')\n",
    "# plot the roc curve for the model\n",
    "plt.plot(fpr, tpr, marker='.')\n",
    "# show the plot\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from IPython import get_ipython\n",
    "get_ipython().run_line_magic('matplotlib', 'inline')\n",
    "###########################################\n",
    "\n",
    "import matplotlib.pyplot as pl\n",
    "import numpy as np\n",
    "from sklearn.model_selection import learning_curve\n",
    "from sklearn.naive_bayes import GaussianNB\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.model_selection import ShuffleSplit, train_test_split\n",
    "##\n",
    "#from sklearn.ensemble import RandomForestClassifier\n",
    "#from sklearn.naive_bayes import GaussianNB\n",
    "#from sklearn.linear_model import LogisticRegression\n",
    "#from sklearn.tree import DecisionTreeClassifier\n",
    "#from sklearn.tree import DecisionTreeRegressor\n",
    "#from sklearn.svm import SVC\n",
    "##\n",
    "\n",
    "def ModelLearning(features, satisfaction_status):\n",
    "    \"\"\" Calculates the performance of several models with varying sizes of training data.\n",
    "        The learning and testing scores for each model are then plotted. \"\"\"\n",
    "\n",
    "    # Create 10 cross-validation sets for training and testing\n",
    "    cv = ShuffleSplit(n_splits = 10, test_size = 0.2, random_state = 0)\n",
    "\n",
    "    # Generate the training set sizes increasing by 50\n",
    "    train_sizes = np.rint(np.linspace(1, X.shape[0]*0.8 - 1, 9)).astype(int)\n",
    "\n",
    "    # Create the figure window\n",
    "    fig = pl.figure(figsize=(10,7))\n",
    "    \n",
    "    # Create a Decision tree regressor\n",
    "    regressor = DecisionTreeRegressor()\n",
    "    \n",
    "    # Calculate the training and testing scores\n",
    "    sizes, train_scores, test_scores = learning_curve(regressor, X, y, \\\n",
    "            cv = cv, train_sizes = train_sizes, scoring = 'r2')\n",
    "    \n",
    "    # Find the mean and standard deviation for smoothing\n",
    "    train_std = np.std(train_scores, axis = 1)\n",
    "    train_mean = np.mean(train_scores, axis = 1)\n",
    "    test_std = np.std(test_scores, axis = 1)\n",
    "    test_mean = np.mean(test_scores, axis = 1)\n",
    "    \n",
    "    # Subplot the learning curve\n",
    "    ax = fig.add_subplot(2, 2, 2)\n",
    "    ax.plot(sizes, train_mean, 'o-', color = 'r', label = 'Training Score')\n",
    "    ax.plot(sizes, test_mean, 'o-', color = 'g', label = 'Testing Score')\n",
    "    ax.fill_between(sizes, train_mean - train_std, \\\n",
    "            train_mean + train_std, alpha = 0.15, color = 'r')\n",
    "    ax.fill_between(sizes, test_mean - test_std, \\\n",
    "            test_mean + test_std, alpha = 0.15, color = 'g')\n",
    "\n",
    "        \n",
    "    # Labels\n",
    "    #ax.set_title('max_depth = %s'%(depth))\n",
    "    ax.set_xlabel('Number of Training Points')\n",
    "    ax.set_ylabel('Score')\n",
    "    ax.set_xlim([0, X.shape[0]*0.8])\n",
    "    ax.set_ylim([-0.05, 1.05])\n",
    "        \n",
    "\n",
    "    # Visual aesthetics\n",
    "    ax.legend(bbox_to_anchor=(1.05, 2.05), loc='lower left', borderaxespad = 0.)\n",
    "    fig.suptitle('Decision Tree Regressor Learning Performances', fontsize = 16, y = 1.03)\n",
    "    fig.tight_layout()\n",
    "    fig.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ModelLearning(features, satisfaction_status)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from IPython import get_ipython\n",
    "get_ipython().run_line_magic('matplotlib', 'inline')\n",
    "###########################################\n",
    "\n",
    "import matplotlib.pyplot as pl\n",
    "import numpy as np\n",
    "from sklearn.model_selection import learning_curve\n",
    "from sklearn.model_selection import validation_curve\n",
    "from sklearn.tree import DecisionTreeRegressor\n",
    "from sklearn.model_selection import ShuffleSplit, train_test_split\n",
    "\n",
    "\n",
    "def ModelLearning(features, satisfaction_status):\n",
    "    \"\"\" Calculates the performance of several models with varying sizes of training data.\n",
    "        The learning and testing scores for each model are then plotted. \"\"\"\n",
    "\n",
    "    # Create 10 cross-validation sets for training and testing\n",
    "    cv = ShuffleSplit(n_splits = 10, test_size = 0.2, random_state = 0)\n",
    "\n",
    "    # Generate the training set sizes increasing by 50\n",
    "    train_sizes = np.rint(np.linspace(1, X.shape[0]*0.8 - 1, 9)).astype(int)\n",
    "\n",
    "    # Create the figure window\n",
    "    fig = pl.figure(figsize=(10,7))\n",
    "\n",
    "    # Create three different models based on max_depth\n",
    "    for k, depth in enumerate([1,3,6,10]):\n",
    "\n",
    "        # Create a Decision tree regressor at max_depth = depth\n",
    "        regressor = DecisionTreeRegressor(max_depth = depth)\n",
    "\n",
    "        # Calculate the training and testing scores\n",
    "        sizes, train_scores, test_scores = learning_curve(regressor, X, y, \\\n",
    "            cv = cv, train_sizes = train_sizes, scoring = 'r2')\n",
    "\n",
    "        # Find the mean and standard deviation for smoothing\n",
    "        train_std = np.std(train_scores, axis = 1)\n",
    "        train_mean = np.mean(train_scores, axis = 1)\n",
    "        test_std = np.std(test_scores, axis = 1)\n",
    "        test_mean = np.mean(test_scores, axis = 1)\n",
    "\n",
    "        # Subplot the learning curve\n",
    "        ax = fig.add_subplot(2, 2, k+1)\n",
    "        ax.plot(sizes, train_mean, 'o-', color = 'r', label = 'Training Score')\n",
    "        ax.plot(sizes, test_mean, 'o-', color = 'g', label = 'Testing Score')\n",
    "        ax.fill_between(sizes, train_mean - train_std, \\\n",
    "            train_mean + train_std, alpha = 0.15, color = 'r')\n",
    "        ax.fill_between(sizes, test_mean - test_std, \\\n",
    "            test_mean + test_std, alpha = 0.15, color = 'g')\n",
    "\n",
    "        # Labels\n",
    "        ax.set_title('max_depth = %s'%(depth))\n",
    "        ax.set_xlabel('Number of Training Points')\n",
    "        ax.set_ylabel('Score')\n",
    "        ax.set_xlim([0, X.shape[0]*0.8])\n",
    "        ax.set_ylim([-0.05, 1.05])\n",
    "\n",
    "    # Visual aesthetics\n",
    "    ax.legend(bbox_to_anchor=(1.05, 2.05), loc='lower left', borderaxespad = 0.)\n",
    "    fig.suptitle('Decision Tree Regressor Learning Performances', fontsize = 16, y = 1.03)\n",
    "    fig.tight_layout()\n",
    "    fig.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ModelLearning(features, satisfaction_status)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Learning Curves\n",
    "\n",
    "\n",
    "A learning curve graph for both training and testing per each model will produce; and the model is scored on both the training and testing sets using R2."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A learning curve shows the validation and training score of an estimator for varying numbers of training samples. It is a tool to find out how much we benefit from adding more training data and whether the estimator suffers more from a variance error or a bias error. If both the validation score and the training score converge to a value that is too low with increasing size of the training set, we will not benefit much from more training data. In the following plot you can see an example: naive Bayes roughly converges to a low score."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from IPython import get_ipython\n",
    "get_ipython().run_line_magic('matplotlib', 'inline')\n",
    "###########################################\n",
    "\n",
    "import matplotlib.pyplot as pl\n",
    "import numpy as np\n",
    "from sklearn.model_selection import learning_curve\n",
    "from sklearn.model_selection import validation_curve\n",
    "#from sklearn.tree import DecisionTreeRegressor\n",
    "from sklearn.model_selection import ShuffleSplit, train_test_split\n",
    "\n",
    "\n",
    "def ModelLearning(features, satisfaction_status):\n",
    "    \"\"\" Calculates the performance of several models with varying sizes of training data.\n",
    "        The learning and testing scores for each model are then plotted. \"\"\"\n",
    "\n",
    "    # Create 10 cross-validation sets for training and testing\n",
    "    cv = ShuffleSplit(n_splits = 10, test_size = 0.25, random_state = 0)\n",
    "\n",
    "    # Generate the training set sizes increasing by 50\n",
    "    train_sizes = np.rint(np.linspace(1, features.shape[0]*0.75 - 1, 9)).astype(int)\n",
    "    \n",
    "    print(train_sizes)\n",
    "\n",
    "    # Create the figure window\n",
    "    fig = pl.figure(figsize=(10,7))\n",
    "\n",
    "    # Create three different models based on max_depth\n",
    "    clf_RF = RandomForestClassifier(random_state=42)\n",
    "    clf_DT = DecisionTreeClassifier()\n",
    "    #clf_LR = LogisticRegression(random_state=42)\n",
    "    clf_NB = GaussianNB()\n",
    "    #clf_SVM = SVC()\n",
    "\n",
    "    \n",
    "    \n",
    "    #clssfiers = [clf_RF] \n",
    "    #clssfiers = [clf_LR] \n",
    "    clssfiers = [clf_RF,clf_DT,clf_NB]  # clf_LR, ,clf_SVM\n",
    "\n",
    "    k=0\n",
    "    for c in clssfiers:\n",
    "        k=k+1\n",
    "        # Create a Decision tree regressor at max_depth = depth\n",
    "        regressor = c\n",
    "\n",
    "        # Calculate the training and testing scores\n",
    "        sizes, train_scores, test_scores = learning_curve(regressor, features, satisfaction_status, \\\n",
    "            cv = cv, train_sizes = train_sizes)  # , scoring = 'r2'\n",
    "\n",
    "        # Find the mean and standard deviation for smoothing\n",
    "        train_std = np.std(train_scores, axis = 1)\n",
    "        train_mean = np.mean(train_scores, axis = 1)\n",
    "        test_std = np.std(test_scores, axis = 1)\n",
    "        test_mean = np.mean(test_scores, axis = 1)\n",
    "\n",
    "        # Subplot the learning curve\n",
    "        ax = fig.add_subplot(2, 3, k)\n",
    "        ax.plot(sizes, train_mean, 'o-', color = 'r', label = 'Training Score')\n",
    "        ax.plot(sizes, test_mean, 'o-', color = 'g', label = 'Testing Score')\n",
    "        ax.fill_between(sizes, train_mean - train_std, \\\n",
    "            train_mean + train_std, alpha = 0.15, color = 'r')\n",
    "        ax.fill_between(sizes, test_mean - test_std, \\\n",
    "            test_mean + test_std, alpha = 0.15, color = 'g')\n",
    "\n",
    "        # Labels\n",
    "        ax.set_title('clssfier = %s'%(k))\n",
    "        ax.set_xlabel('Number of Training Points')\n",
    "        ax.set_ylabel('Score')\n",
    "        ax.set_xlim([0, features.shape[0]*0.8])\n",
    "        ax.set_ylim([-0.05, 1.05])\n",
    "\n",
    "    # Visual aesthetics\n",
    "    ax.legend(bbox_to_anchor=(1.05, 2.05), loc='lower left', borderaxespad = 0.)\n",
    "    fig.suptitle('Clssfiers Learning Performances', fontsize = 16, y = 1.03)\n",
    "    fig.tight_layout()\n",
    "    fig.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ModelLearning(features, satisfaction_status)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
